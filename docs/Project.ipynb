{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98fdc7c",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Mini-Project 2: Scientific Abstracts Retrieval (arXiv)\n",
    "\n",
    "**Course:** Techniques d'Indexation et de RÃ©fÃ©rencement\n",
    "**Objective:** Build an IR system using Vector Space Model, TF-IDF, and Rocchio Relevance Feedback.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Imports and Setup\n",
    "Run this cell to import necessary libraries and download NLTK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd97770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Setup NLTK\n",
    "def download_nltk_dependencies():\n",
    "    resources = ['stopwords', 'punkt', 'punkt_tab']\n",
    "    print(\"Checking NLTK dependencies...\")\n",
    "    for res in resources:\n",
    "        nltk.download(res, quiet=True)\n",
    "\n",
    "download_nltk_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9751bf",
   "metadata": {},
   "source": [
    "### 2. Configuration & Constants\n",
    "Here we define the dataset paths, algorithm parameters, and the test queries with their ground truth categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65b4514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "JSON_FILE = 'arxiv-metadata-oai-snapshot.json'\n",
    "CSV_FILE = 'arxiv_sample.csv'\n",
    "INDEX_FILE = 'inverted_index.json'\n",
    "\n",
    "MAX_ROWS = 10000        # Data limit\n",
    "TOP_K_RESULTS = 10      # Rank cut-off\n",
    "\n",
    "# Rocchio Algorithm Weights\n",
    "ROCCHIO_ALPHA = 1.0     # Original query weight\n",
    "ROCCHIO_BETA = 0.75     # Relevant docs weight\n",
    "ROCCHIO_GAMMA = 0.15    # Non-relevant docs weight\n",
    "\n",
    "# Test Queries\n",
    "QUERIES = [\n",
    "    \"quantum field theory\",\n",
    "    \"semiconductor laser\",\n",
    "    \"graph neural network\",\n",
    "    \"cosmic microwave background\",\n",
    "    \"optical cavity\",\n",
    "    \"spintronics\",\n",
    "    \"superconducting qubits\",\n",
    "    \"photonic integrated circuits\"\n",
    "]\n",
    "\n",
    "# Relevance Judgment (Ground Truth)\n",
    "QUERY_CATEGORY_MAP = {\n",
    "    \"quantum field theory\": [\"hep-th\", \"quant-ph\"],\n",
    "    \"semiconductor laser\": [\"physics.optics\", \"cond-mat\"],\n",
    "    \"graph neural network\": [\"cs.LG\", \"cs.AI\", \"stat.ML\"],\n",
    "    \"cosmic microwave background\": [\"astro-ph\"],\n",
    "    \"optical cavity\": [\"physics.optics\", \"quant-ph\"],\n",
    "    \"spintronics\": [\"cond-mat\"],\n",
    "    \"superconducting qubits\": [\"quant-ph\"],\n",
    "    \"photonic integrated circuits\": [\"physics.optics\", \"eess.IV\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ed5a8",
   "metadata": {},
   "source": [
    "### 3. Data Loading\n",
    "This step reads the raw JSON dataset (if available) or loads a pre-processed CSV. It combines the `Title` and `Abstract` into a single text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7704cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_dataset():\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        print(f\"Loading existing dataset from {CSV_FILE}...\")\n",
    "        df = pd.read_csv(CSV_FILE)\n",
    "    else:\n",
    "        print(f\"CSV not found. Generating sample from {JSON_FILE} (Max {MAX_ROWS} rows)...\")\n",
    "        if not os.path.exists(JSON_FILE):\n",
    "            # Fallback if file doesn't exist (for demo purposes)\n",
    "            print(\"âš ï¸ JSON file not found. Please ensure 'arxiv-metadata-oai-snapshot.json' is in the folder.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        rows = []\n",
    "        with open(JSON_FILE, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= MAX_ROWS: \n",
    "                    break\n",
    "                d = json.loads(line)\n",
    "                rows.append((d['title'], d['abstract'], d['categories']))\n",
    "        \n",
    "        df = pd.DataFrame(rows, columns=['title', 'abstract', 'categories'])\n",
    "        df.to_csv(CSV_FILE, index=False)\n",
    "        print(f\"Dataset saved to {CSV_FILE}\")\n",
    "\n",
    "    # Pre-processing\n",
    "    df['text'] = (df['title'].fillna('') + '. ' + df['abstract'].fillna(''))\n",
    "    return df\n",
    "\n",
    "# Execute Load\n",
    "df = load_or_create_dataset()\n",
    "docs = df['text'].tolist()\n",
    "print(f\"Loaded {len(docs)} documents.\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbdb3cd",
   "metadata": {},
   "source": [
    "### 4. Task 1: Inverted Index\n",
    "We build a manual inverted index to map terms to document IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2977280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index(docs):\n",
    "    print(\"Building Inverted Index...\")\n",
    "    inverted_index = defaultdict(dict)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    for doc_id, text in enumerate(docs):\n",
    "        tokens = tokenizer.tokenize(text.lower())\n",
    "        for token in tokens:\n",
    "            if token not in stop_words and len(token) > 2:\n",
    "                if doc_id not in inverted_index[token]:\n",
    "                    inverted_index[token][doc_id] = 0\n",
    "                inverted_index[token][doc_id] += 1\n",
    "    \n",
    "    with open(INDEX_FILE, 'w') as f:\n",
    "        json.dump(inverted_index, f)\n",
    "    print(f\"Inverted index saved to {INDEX_FILE} ({len(inverted_index)} terms)\")\n",
    "\n",
    "# Build the index (Optional: comment out if already exists)\n",
    "build_inverted_index(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94959be",
   "metadata": {},
   "source": [
    "### 5. Vector Space Model (TF-IDF)\n",
    "We convert the text documents into numerical vectors using Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c23ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vectorizing Documents (TF-IDF)...\")\n",
    "# Standard configuration: English stopwords, min document frequency of 3\n",
    "vec = TfidfVectorizer(stop_words='english', min_df=3)\n",
    "X = vec.fit_transform(docs)\n",
    "print(f\"Vocabulary size: {len(vec.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ea7cc",
   "metadata": {},
   "source": [
    "### 6. Tasks 2, 3 & 4: Search & Evaluation Logic\n",
    "Definitions for the search engine (Cosine Similarity) and evaluation metrics (P@K, MAP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d590b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_search(query, vectorizer, X, k=TOP_K_RESULTS):\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_vec, X).ravel()\n",
    "    top_indices = np.argsort(similarities)[::-1][:k]\n",
    "    return list(zip(top_indices, similarities[top_indices]))\n",
    "\n",
    "def is_relevant(doc_categories, query_text):\n",
    "    target_cats = QUERY_CATEGORY_MAP.get(query_text, [])\n",
    "    if not target_cats: return False\n",
    "    doc_cat_list = str(doc_categories).split()\n",
    "    for t_cat in target_cats:\n",
    "        if t_cat in doc_cat_list: return True\n",
    "    return False\n",
    "\n",
    "def calculate_metrics(results, query, df, k=TOP_K_RESULTS):\n",
    "    relevant_count = 0\n",
    "    precisions = []\n",
    "    \n",
    "    for rank, (doc_idx, score) in enumerate(results):\n",
    "        doc_cats = df.iloc[doc_idx]['categories']\n",
    "        if is_relevant(doc_cats, query):\n",
    "            relevant_count += 1\n",
    "            current_precision = relevant_count / (rank + 1)\n",
    "            precisions.append(current_precision)\n",
    "            \n",
    "    p_at_k = relevant_count / k\n",
    "    ap = 0.0 if relevant_count == 0 else sum(precisions) / relevant_count\n",
    "    return p_at_k, ap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a011b95a",
   "metadata": {},
   "source": [
    "### 7. Task 5: Rocchio Relevance Feedback\n",
    "This function refines the query vector based on the initial search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc448daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rocchio_update(query_vec, X, top_indices, doc_categories, query_text):\n",
    "    relevant_indices = []\n",
    "    non_relevant_indices = []\n",
    "    \n",
    "    for idx in top_indices:\n",
    "        cats = doc_categories.iloc[idx]\n",
    "        if is_relevant(cats, query_text):\n",
    "            relevant_indices.append(idx)\n",
    "        else:\n",
    "            non_relevant_indices.append(idx)\n",
    "            \n",
    "    q_new = ROCCHIO_ALPHA * query_vec\n",
    "    \n",
    "    if relevant_indices:\n",
    "        Dpos = X[relevant_indices]\n",
    "        mean_pos = np.array(Dpos.mean(axis=0))\n",
    "        q_new = q_new + ROCCHIO_BETA * mean_pos\n",
    "        \n",
    "    if non_relevant_indices:\n",
    "        Dneg = X[non_relevant_indices]\n",
    "        mean_neg = np.array(Dneg.mean(axis=0))\n",
    "        q_new = q_new - ROCCHIO_GAMMA * mean_neg\n",
    "        \n",
    "    q_new[q_new < 0] = 0\n",
    "    return q_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8bdf4",
   "metadata": {},
   "source": [
    "### 8. Experiment: Standard Search vs. Rocchio Feedback\n",
    "We now run the experiment on the 8 queries and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86517a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Executing Queries (Standard vs Rocchio) ===\")\n",
    "print(f\"{'Query':<30} | {'P@10 (Base)':<12} | {'P@10 (Rocchio)':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "map_base_scores = []\n",
    "map_rocchio_scores = []\n",
    "\n",
    "for q in QUERIES:\n",
    "    # 1. Base Search\n",
    "    results_base = perform_search(q, vec, X, k=TOP_K_RESULTS)\n",
    "    p10_base, ap_base = calculate_metrics(results_base, q, df, k=TOP_K_RESULTS)\n",
    "    map_base_scores.append(ap_base)\n",
    "    \n",
    "    # 2. Rocchio Feedback\n",
    "    q_vec = vec.transform([q])\n",
    "    top_indices = [idx for idx, score in results_base]\n",
    "    \n",
    "    # Compute new query vector\n",
    "    q_vec_new = rocchio_update(q_vec, X, top_indices, df['categories'], q)\n",
    "    \n",
    "    # Search with new vector (manual similarity calc since q_vec_new is already vector)\n",
    "    sims_new = cosine_similarity(np.asarray(q_vec_new), X).ravel()\n",
    "    top_indices_new = np.argsort(sims_new)[::-1][:TOP_K_RESULTS]\n",
    "    results_rocchio = list(zip(top_indices_new, sims_new[top_indices_new]))\n",
    "    \n",
    "    # 3. Evaluate Rocchio\n",
    "    p10_rocchio, ap_rocchio = calculate_metrics(results_rocchio, q, df, k=TOP_K_RESULTS)\n",
    "    map_rocchio_scores.append(ap_rocchio)\n",
    "    \n",
    "    print(f\"{q[:30]:<30} | {p10_base:.4f}       | {p10_rocchio:.4f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Mean Average Precision (MAP) Base:    {np.mean(map_base_scores):.4f}\")\n",
    "print(f\"Mean Average Precision (MAP) Rocchio: {np.mean(map_rocchio_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56768d14",
   "metadata": {},
   "source": [
    "### 9. Task 6: Ablation Study\n",
    "Comparing different preprocessing pipelines (Stopwords/Stemming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9151aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [stemmer.stem(t) for t in tokens if t.isalnum()]\n",
    "\n",
    "def run_ablation(df, queries):\n",
    "    print(\"\\n=== Running Ablation Study (Task 6) ===\")\n",
    "    \n",
    "    pipelines = [\n",
    "        (\"No Stop, No Stem\", None, None),\n",
    "        (\"With Stop, No Stem\", 'english', None),\n",
    "        (\"No Stop, With Stem\", None, custom_tokenizer),\n",
    "        (\"With Stop, With Stem\", 'english', custom_tokenizer)\n",
    "    ]\n",
    "    \n",
    "    results_table = []\n",
    "\n",
    "    for name, stop_w, tokenizer_func in pipelines:\n",
    "        print(f\"Training Pipeline: {name}...\")\n",
    "        \n",
    "        vec_ablation = TfidfVectorizer(stop_words=stop_w, tokenizer=tokenizer_func, min_df=3)\n",
    "        \n",
    "        try:\n",
    "            X_ablation = vec_ablation.fit_transform(df['text'])\n",
    "        except ValueError:\n",
    "            print(f\"Skipping {name}: Empty vocabulary.\")\n",
    "            continue\n",
    "            \n",
    "        map_scores = []\n",
    "        for q in queries:\n",
    "            results = perform_search(q, vec_ablation, X_ablation, k=TOP_K_RESULTS)\n",
    "            _, ap = calculate_metrics(results, q, df, k=TOP_K_RESULTS)\n",
    "            map_scores.append(ap)\n",
    "            \n",
    "        mean_map = np.mean(map_scores)\n",
    "        results_table.append((name, mean_map))\n",
    "        \n",
    "    print(\"\\n--- Ablation Results (MAP) ---\")\n",
    "    print(f\"{'Pipeline':<25} | {'MAP':<10}\")\n",
    "    print(\"-\" * 37)\n",
    "    for name, score in results_table:\n",
    "        print(f\"{name:<25} | {score:.4f}\")\n",
    "\n",
    "run_ablation(df, QUERIES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
